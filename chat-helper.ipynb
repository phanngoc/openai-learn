{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cleantext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfire\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcleantext\u001b[39;00m \u001b[39mimport\u001b[39;00m clean\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m AVAILABLE_MODELS \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgpt-4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgpt-4-0314\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcode-davinci-002\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/openai-research/chat-helper.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cleantext'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "inference_openai.py - text generation with OpenAI API\n",
    "    See https://platform.openai.com/docs/quickstart for more details.\n",
    "Usage:\n",
    "python inference_openai.py --prompt \"The quick brown fox jumps over the lazy dog.\" --model \"gpt-3.5-turbo\" --temperature 0.5 --max_tokens 256 --n 1 --stop \".\"\n",
    "Detailed usage:\n",
    "python inference_openai.py --help\n",
    "Notes:\n",
    "- The OpenAI API key can be set using the OPENAI_API_KEY environment variable (recommended) or using the --api_key argument.\n",
    "- This script supports inference with the \"chat\" models only.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import fire\n",
    "import openai\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "AVAILABLE_MODELS = [\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-32k\",\n",
    "    \"gpt-4-32k-0314\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"text-davinci-003\",\n",
    "    \"code-davinci-002\",\n",
    "]\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%b/%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "\n",
    "def validate_model(model):\n",
    "    \"\"\"\n",
    "    Validates the given model name against the list of available models (see AVAILABLE_MODELS)\n",
    "        NOTE: this does not mean you have access to the model, just a basic check.\n",
    "    :param model: The name of the model to validate.\n",
    "    :raises ValueError: If the given model is not in the list of available models.\n",
    "    \"\"\"\n",
    "    if model not in AVAILABLE_MODELS:\n",
    "        raise ValueError(\n",
    "            f\"Invalid model '{model}', available models: {', '.join(AVAILABLE_MODELS)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def chat_generate_text(\n",
    "    prompt: str,\n",
    "    openai_api_key: str = None,\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    system_prompt: str = \"You are a helpful assistant.\",\n",
    "    temperature: float = 0.5,\n",
    "    max_tokens: int = 256,\n",
    "    n: int = 1,\n",
    "    stop: Optional[Union[str, list]] = None,\n",
    "    presence_penalty: float = 0,\n",
    "    frequency_penalty: float = 0.1,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    chat_generate_text - Generates text using the OpenAI API.\n",
    "    :param str prompt: prompt for the model\n",
    "    :param str openai_api_key: api key for the OpenAI API, defaults to None\n",
    "    :param str model: model to use, defaults to \"gpt-3.5-turbo\"\n",
    "    :param str system_prompt: initial prompt for the model, defaults to \"You are a helpful assistant.\"\n",
    "    :param float temperature: _description_, defaults to 0.5\n",
    "    :param int max_tokens: _description_, defaults to 256\n",
    "    :param int n: _description_, defaults to 1\n",
    "    :param Optional[Union[str, list]] stop: _description_, defaults to None\n",
    "    :param float presence_penalty: _description_, defaults to 0\n",
    "    :param float frequency_penalty: _description_, defaults to 0.1\n",
    "    :return List[str]: _description_\n",
    "    \"\"\"\n",
    "    if openai_api_key is None:\n",
    "        openai_api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "    assert openai_api_key is not None, \"OpenAI API key not found.\"\n",
    "\n",
    "    openai.api_key = openai_api_key\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        n=n,\n",
    "        stop=stop,\n",
    "        presence_penalty=presence_penalty,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "\n",
    "    generated_texts = [\n",
    "        choice.message[\"content\"].strip() for choice in response[\"choices\"]\n",
    "    ]\n",
    "    return generated_texts\n",
    "\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Returns the current timestamp in the format YYYYMMDD_HHMM\"\"\"\n",
    "    return datetime.now().strftime(\"%Y%b%d_%H-%M\")\n",
    "\n",
    "\n",
    "def read_and_clean_file(file_path, lower=False):\n",
    "    \"\"\"\n",
    "    Reads the content of a file and cleans the text using the cleantext package.\n",
    "    :param file_path: The path to the file.\n",
    "    :return: The cleaned text.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        context = clean(f.read(), lower=lower)\n",
    "    return context\n",
    "\n",
    "\n",
    "def save_output_to_file(\n",
    "    out_dir,\n",
    "    output,\n",
    "    file_name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves the generated output to a file.\n",
    "    :param out_dir: The output directory.\n",
    "    :param output: The text to be saved.\n",
    "    :param file_name: The name of the output file.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = out_dir / file_name\n",
    "\n",
    "    with output_file.open(\"w\") as f:\n",
    "        f.write(output)\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    prompt: Optional[str] = None,\n",
    "    api_key: Optional[str] = None,\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    system_prompt: str = \"You are a helpful assistant.\",\n",
    "    temperature: float = 0.5,\n",
    "    max_tokens: int = 256,\n",
    "    n: int = 1,\n",
    "    stop: Optional[Union[str, list]] = None,\n",
    "    presence_penalty: float = 0.0,\n",
    "    frequency_penalty: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to run the text generation script.\n",
    "    :param prompt: The input prompt for the model.\n",
    "    :param api_key: The OpenAI API key. If not provided, checks the environment variable OPENAI_API_KEY.\n",
    "    :param model: openai model code, defaults to \"gpt-3.5-turbo\"\n",
    "    :param system_prompt: The system prompt for the model, defaults to \"You are a helpful assistant.\"\n",
    "    :param temperature: The sampling temperature (creativity) for the model. (default: 0.5)\n",
    "    :param max_tokens: The maximum number of tokens in the generated text. (default: 256)\n",
    "    :param n: The number of generated texts. (default: 1)\n",
    "    :param stop: The stopping sequence(s) for the model. (default: None)\n",
    "    :param presence_penalty: The penalty applied for new token presence. (default: 0.0)\n",
    "    :param frequency_penalty: The penalty applied based on token frequency. (default: 0.0)\n",
    "    :param file_path: The path to a file/directory to include after the prompt.\n",
    "    :param out_dir: directory to save outputs. (default: parent directory of input path if provided)\n",
    "    :param save_prompt: Save the input prompt in the output files with the generated text. (default: False)\n",
    "    :param markdown: save the generated text as a markdown file. (default: False)\n",
    "    :param verbose: Whether to print the generated text to the console.\n",
    "    \"\"\"\n",
    "    openai.api_key = api_key if api_key else os.getenv(\"OPENAI_API_KEY\")\n",
    "    assert (\n",
    "        openai.api_key is not None\n",
    "    ), \"API key not found - pass as arg or set environment variable OPENAI_API_KEY\"\n",
    "\n",
    "    prompt = prompt.strip()\n",
    "    validate_model(model)\n",
    "\n",
    "    generated_texts = chat_generate_text(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        system_prompt=system_prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        n=n,\n",
    "        stop=stop,\n",
    "        presence_penalty=presence_penalty,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "print(f\"Generated text: {generated_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKKT Vân Phong cũng được định hướng để trở thành khu vực phát triển đô thị thông minh bền vững với hệ thống cơ sở hạ tầng kinh tế - xã hội phát triển đồng bộ, hiện đại; có vị trí quan trọng về an ninh, quốc phòng của quốc gia.Đáng chú ý, theo quy hoạch, khu vực dự kiến phát triển cảng hàng không khoảng 500 ha tại xã Vạn Thắng, H.Vạn Ninh - khu vực phía bắc Vân Phong. Các khu vực phát triển sân golf được bố trí tại đảo Hòn Lớn, khu Đầm Môn, khu vực Tuần Lễ - Hòn Ngang.Mới đây, tại cuộc họp góp ý hoàn chỉnh Đề án quy hoạch Cảng hàng không Vân Phong, Chủ tịch UBND tỉnh Khánh Hòa Nguyễn Tấn Tuân đã giao Sở GTVT phối hợp với đơn vị tư vấn khẩn trương hoàn chỉnh đề án.Ông Tuân cũng đề nghị đơn vị tư vấn phải làm rõ được nguồn cung cấp vật liệu phục vụ dự án; bổ sung kho chứa xăng dầu trong cảng theo quy mô, thẩm quyền phê duyệt; bổ sung kho chứa hàng hóa, các khu vực tác nghiệp, sửa chữa máy bay, đồng thời đánh giá, làm rõ hơn hiệu quả đầu tư dự án.\n",
      "        (runinit = window.runinit || []).push(function () {\n",
      "                if (typeof _chkPrLink != 'undefined' && _chkPrLink)\n",
      "            return;\n",
      "\n",
      "        var mutexAds = '<zone id=\"l2srqb41\"></zone>';\n",
      "        var content = $('[data-role=\"content\"]');\n",
      "        if (content.length > 0) {\n",
      "            var childNodes = content[0].childNodes;\n",
      "            for (i = 0; i < childNodes.length; i++) {\n",
      "                var childNode = childNodes[i];\n",
      "\n",
      "                var isPhotoOrVideo = false;\n",
      "                if (childNode.nodeName.toLowerCase() == 'div') {\n",
      "                                        var type = $(childNode).attr('class') + '';\n",
      "\n",
      "                    if (type.indexOf('VCSortableInPreviewMode') >= 0) {\n",
      "                        isPhotoOrVideo = true;\n",
      "                    }\n",
      "                }\n",
      "\n",
      "                try {\n",
      "                    if ((i >= childNodes.length / 2 - 1) && (i < childNodes.length / 2) && !isPhotoOrVideo) {\n",
      "                        if (i <= childNodes.length - 3) {\n",
      "                            childNode.after(htmlToElement(mutexAds));\n",
      "                            arfAsync.push(\"l2srqb41\");\n",
      "                        }\n",
      "                        break;\n",
      "                    }\n",
      "                }\n",
      "                catch (e) { }\n",
      "            }\n",
      "        }\n",
      "    });\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_and_trim(text):\n",
    "    # Remove JavaScript code\n",
    "    text_without_js = re.sub(r'\\/\\/.*?(\\n|$)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Trim leading and trailing whitespaces\n",
    "    trimmed_text = text_without_js.strip()\n",
    "    \n",
    "    return trimmed_text\n",
    "\n",
    "# Example text\n",
    "vietnamese_text_with_js = \"\"\"\n",
    "QKKT Vân Phong cũng được định hướng để trở thành khu vực phát triển đô thị thông minh bền vững với hệ thống cơ sở hạ tầng kinh tế - xã hội phát triển đồng bộ, hiện đại; có vị trí quan trọng về an ninh, quốc phòng của quốc gia.Đáng chú ý, theo quy hoạch, khu vực dự kiến phát triển cảng hàng không khoảng 500 ha tại xã Vạn Thắng, H.Vạn Ninh - khu vực phía bắc Vân Phong. Các khu vực phát triển sân golf được bố trí tại đảo Hòn Lớn, khu Đầm Môn, khu vực Tuần Lễ - Hòn Ngang.Mới đây, tại cuộc họp góp ý hoàn chỉnh Đề án quy hoạch Cảng hàng không Vân Phong, Chủ tịch UBND tỉnh Khánh Hòa Nguyễn Tấn Tuân đã giao Sở GTVT phối hợp với đơn vị tư vấn khẩn trương hoàn chỉnh đề án.Ông Tuân cũng đề nghị đơn vị tư vấn phải làm rõ được nguồn cung cấp vật liệu phục vụ dự án; bổ sung kho chứa xăng dầu trong cảng theo quy mô, thẩm quyền phê duyệt; bổ sung kho chứa hàng hóa, các khu vực tác nghiệp, sửa chữa máy bay, đồng thời đánh giá, làm rõ hơn hiệu quả đầu tư dự án.\n",
    "    //Chèn ads giữa bài\n",
    "    (runinit = window.runinit || []).push(function () {\n",
    "        //Nếu k chạy ads thì return\n",
    "        if (typeof _chkPrLink != 'undefined' && _chkPrLink)\n",
    "            return;\n",
    "\n",
    "        var mutexAds = '<zone id=\"l2srqb41\"></zone>';\n",
    "        var content = $('[data-role=\"content\"]');\n",
    "        if (content.length > 0) {\n",
    "            var childNodes = content[0].childNodes;\n",
    "            for (i = 0; i < childNodes.length; i++) {\n",
    "                var childNode = childNodes[i];\n",
    "\n",
    "                var isPhotoOrVideo = false;\n",
    "                if (childNode.nodeName.toLowerCase() == 'div') {\n",
    "                    // kiem tra xem co la anh khong?\n",
    "                    var type = $(childNode).attr('class') + '';\n",
    "\n",
    "                    if (type.indexOf('VCSortableInPreviewMode') >= 0) {\n",
    "                        isPhotoOrVideo = true;\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                try {\n",
    "                    if ((i >= childNodes.length / 2 - 1) && (i < childNodes.length / 2) && !isPhotoOrVideo) {\n",
    "                        if (i <= childNodes.length - 3) {\n",
    "                            childNode.after(htmlToElement(mutexAds));\n",
    "                            arfAsync.push(\"l2srqb41\");\n",
    "                        }\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "                catch (e) { }\n",
    "            }\n",
    "        }\n",
    "    });\n",
    "\"\"\"\n",
    "\n",
    "# Parse and trim the text\n",
    "trimmed_text = parse_and_trim(vietnamese_text_with_js)\n",
    "\n",
    "# Print the result\n",
    "print(trimmed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
